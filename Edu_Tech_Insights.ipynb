{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Education Technology Firm Business Analysis\n",
    "### Ziling Huang\n",
    "\n",
    "## Introduction\n",
    "The goal of this report is to create a data pipeline using clients assessments data. The data will be used to provide insights to the data scientists for our clients to support product strategy and development decisions.\n",
    "\n",
    "## Contents\n",
    "#### 1. Step-by-Step Approach to Building the Pipeline   \n",
    "\n",
    "#### 2. Business Questions\n",
    "\n",
    "#### 3. Data Exploration : Steps and Assumptions\n",
    "\n",
    "#### 4. Data Insights : Conclusion and Recommendations\n",
    "\n",
    "#### 5. Data Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Step-by-Step Approach to Building the Pipeline  \n",
    "\n",
    "Event logs from client assessments are stored in a nested json file called \"assessments-attempts-20180128-121051-nested.json\". I created a pipeline and sent the data record as one message in Kafka to publish and consume messages on topics for the users. Next, the data is read into Pyspark and transformed into a SQL query friendly format.\n",
    "\n",
    "<b> Step 1 </b> : Spin up a docker container cluster using the following in Command line:  \n",
    "\n",
    "`docker-compose up -d`  \n",
    "\n",
    "This spins up a container cluster using the configuration specifications in the docker-compose.yml file and -d runs it in the background. The yml file contains ports for Zookeeper - resource manager, Kafka - Data manager, Cloudera Hadoop - HDFS  for Data Storage , Spark for Data transformation. \n",
    "\n",
    "<b> Step 2 </b> : Next I check the logs to make sure there is no error :\n",
    "\n",
    "`docker-compose logs -f kafka`\n",
    "\n",
    "<b> Step 3 </b> : Create a topic \"assessments\" to help store data for Kafka to pick up and send for processing :\n",
    "\n",
    "`docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181`\n",
    "\n",
    "\n",
    "<b> Step 4 </b> : Using curl I retrieve the json file and write it to the skh folder:\n",
    "\n",
    "`curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp`\n",
    "\n",
    "\n",
    "<b> Step 5 </b>: Read the data from the json file through jq formatting and then send it to Kafka cat in producer mode to produce message and send them to Kafka\n",
    "\n",
    "`docker-compose exec mids bash -c \"cat /w205/project-2-zilingh8/skh/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments\"`\n",
    "\n",
    " \n",
    "<b> Step 6 </b>: Spin up pyspark to pick up the information from Kafka and transform data :\n",
    "\n",
    " `docker-compose exec spark pyspark`\n",
    " \n",
    "<b> Step 7 </b> : Read data from Kafka from start to end and load in Pyspark :\n",
    "\n",
    "`raw_assess=spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\",\"kafka:29092\").option(\"subscribe\",\"assessments\").option(\"startingOffsets\",\"earliest\").option(\"endingOffsets\",\"latest\").load()`\n",
    " \n",
    " \n",
    "<b> Step 8 </b> : Save cache of the data in Pyspark to suppress warnings. This is because Spark typically does lazy evaluations and caching forces spark to touch all the data:\n",
    "\n",
    "`raw_assess.cache()`\n",
    " \n",
    "<b> Step 9 </b> : Review the data schema :  \n",
    "\n",
    "`raw_assess.printSchema()`\n",
    "\n",
    "<b> Step 10 </b> : Retrieve the value in the data and cast it as string:\n",
    "\n",
    "`assess_s= raw_assess.select(raw_assess.value.cast('string'))`\n",
    "\n",
    "<b> Step 11 </b> : Write the data to HDFS in parquet format:\n",
    "\n",
    "`assess_s.write.parquet(\"/tmp/assessments\")`\n",
    "\n",
    "<b> Step 12 </b> : Use library for linux work to switch file to uni encoding :\n",
    "\n",
    "`import sys\n",
    "sys.stdout=open(sys.stdout.fileno(),mode='w',encoding='utf8',buffering=1)\n",
    "import json `\n",
    "\n",
    "<b> Step 13 </b>: From spark sql use row function. Get RDD distributed datset format and apply a map to it and then convert it back into a dataframe\n",
    "\n",
    "`from pyspark.sql import Row\n",
    "extracted_assess= assess_s.rdd.map(lambda x : json.loads(x.value)).toDF()\n",
    "extracted_assess.show()`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Business Questions\n",
    "\n",
    "The goal of this report is to answer a few business questions, namely:\n",
    "\n",
    "1. What constitutes an assessment and how many assessments are there in this dataset?\n",
    "2. What is the total number of unique exam types available on the portal?\n",
    "3. What are the top 5 most popular exams?\n",
    "4. What are the bottom 5 least popular exams?\n",
    "5. What is the time period range of the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a Data Exploration : Steps\n",
    "\n",
    "<b>Step 1 </b>: Create a spark temp table to query extracted data for data exploration\n",
    "\n",
    "`extracted_assess.registerTempTable('assess_temptab')`\n",
    "\n",
    "<b> Step 2 </b> : Write spark sql to query data and understand relationship between ids and exam_names and what constitutes one assessment\n",
    "\n",
    "`spark.sql(\"select base_exam_id, exam_name, user_exam_id, keen_id from assess_temptab limit 10\").show()`\n",
    "\n",
    "Count number of records to get number of assessments provided on our portal.\n",
    "Answer is 3,280 assessments records taken during this time period.\n",
    "\n",
    "`spark.sql(\"select count(user_exam_id) from assess_temptab\").show()`\n",
    "\n",
    "<div align=\"left\">\n",
    "<table>\n",
    "    \n",
    "|count(user_exam_id)|\n",
    "|-------------------|\n",
    "|               3280|\n",
    "    \n",
    "</table>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "Check if each user_exam_id is a unique assessment. The number of distinct user_exam_ids is 3242 and is lower than the total number of records. \n",
    " \n",
    " `spark.sql(\"select count(distinct(user_exam_id)) from assess_temptab\").show()`\n",
    "                                                 \n",
    "|count(DISTINCT user_exam_id)|\n",
    "|----------------------------|\n",
    "|                        3242|\n",
    "\n",
    "\n",
    "Find records where user_exam_id is more than 1 and examine them:\n",
    "\n",
    "\n",
    "`d1=spark.sql(\"select exam_name, user_exam_id, count(user_exam_id) as count_uaid from assess_temptab GROUP BY exam_name, user_exam_id\")\n",
    "d1.registerTempTable('d2')\n",
    "spark.sql(\"select * from d2 where count_uaid > 1 \").show()`\n",
    "                         \n",
    "|           exam_name|        user_exam_id|count_uaid|\n",
    "|--------------------|--------------------|----------|\n",
    "|Learning C# Best ...|3d63ec69-8d97-4f9...|         3|\n",
    "|Learning C# Best ...|a45b5ee6-a4ed-4b1...|         3|\n",
    "|Learning C# Desig...|fa23b287-0d0a-468...|         3|\n",
    "|An Introduction t...|d4ab4aeb-1368-486...|         3|\n",
    "|Intermediate C# P...|028ad26f-a89f-4a6...|         3|\n",
    "|        Learning DNS|bd96cfbe-1532-4ba...|         3|\n",
    "|Intermediate Pyth...|6e4889ab-5978-44b...|         2|\n",
    "|Beginning C# Prog...|a244c11a-d890-4e3...|         3|\n",
    "|Introduction to B...|b7ac6d15-97e1-4e9...|         3|\n",
    "|Beginning C# Prog...|00745aef-f3af-412...|         3|\n",
    "|Intermediate Pyth...|c1eb4d4a-d6ef-43e...|         2|\n",
    "|Learning C# Best ...|ac80a11a-2e79-40e...|         3|\n",
    "|Beginning C# Prog...|66d91177-c436-4ee...|         3|\n",
    "|Learning C# Desig...|1e325cc1-47a9-480...|         3|\n",
    "|        Learning Git|a7e6fc04-245f-4e3...|         3|\n",
    "|Beginning C# Prog...|c320d47f-60d4-49a...|         3|\n",
    "|Beginning C# Prog...|37cf5b0c-4807-421...|         3|\n",
    "|Intermediate C# P...|949aa36c-74c7-4fc...|         3|\n",
    "|Beginning C# Prog...|6132da16-2c0c-436...|         3|\n",
    "|        Learning Git|cdc5859d-b332-4fb...|         3|\n",
    "\n",
    "\n",
    "The data does not contain a unique record identifier and this is an issue because there could be duplicate records. An in-depth discussion on the needed assumptions this creates will provided in the assumption section below.\n",
    "\n",
    "<b> Step 3 </b> : Use spark sql to understand how many unique exam names (courses) there are:\n",
    "\n",
    "`spark.sql(\"select count(distinct(exam_name)) from assess_temptab\").show()`\n",
    "\n",
    "Answer is 103 unique exam names\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b> Step 4 </b> : Find the most popular exams and least popular exams.\n",
    "\n",
    "Create a table and save to temp table :\n",
    "\n",
    "`t1=spark.sql(\"select distinct(exam_name), count(exam_name) as count from assess_temptab GROUP BY exam_name\")\n",
    "t1.registerTempTable('t2')`\n",
    "\n",
    "Show this table in descending order :\n",
    "\n",
    "`spark.sql(\"select exam_name,count from t2 ORDER BY count desc\").show()`\n",
    "\n",
    "                                                  \n",
    "|           exam_name|count|\n",
    "|--------------------|-----|\n",
    "|        Learning Git|  394|\n",
    "|Introduction to P...|  162|\n",
    "|Introduction to J...|  158|\n",
    "|Intermediate Pyth...|  158|\n",
    "|Learning to Progr...|  128|\n",
    "\n",
    "\n",
    "Show in ascending order:\n",
    "\n",
    "`spark.sql(\"select exam_name,count from t2 ORDER BY count\").show()`\n",
    "\n",
    "                                                   \n",
    "|           exam_name|count|\n",
    "|--------------------|-----|\n",
    "|Learning to Visua...|    1|\n",
    "|Native Web Apps f...|    1|\n",
    "|Nulls, Three-valu...|    1|\n",
    "|Operating Red Hat...|    1|\n",
    "|The Closed World ...|    2|\n",
    "\n",
    "\n",
    "Show how many times people took the foundational course \"Learning Git\":\n",
    "\n",
    "`spark.sql(\"select exam_name, count from t2 WHERE exam_name='Learning Git'\").show()`\n",
    "                                                         \n",
    "|   exam_name|count|\n",
    "|------------|-----|\n",
    "|Learning Git|  394|\n",
    "\n",
    "\n",
    "\n",
    "<b> Step 5 </b> : Check the date time duration of the dataset\n",
    " \n",
    "`c2= extracted_assess.select(extracted_assess.started_at.cast('timestamp'))\n",
    " c2.registerTempTable('c3')\n",
    " spark.sql(\"select min(started_at),max(started_at) from c3\").show()`\n",
    " \n",
    "\n",
    "|     min(started_at)|     max(started_at)|\n",
    "|--------------------|--------------------|\n",
    "|2017-11-21 00:42:...|2018-01-28 19:17:...|\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b Data Exploration : Assumptions\n",
    "\n",
    "<b> Assumption </b> : There were only 3,242 assessments taken over the period from Nov 11, 2017 to Jan 28, 2018.\n",
    "\n",
    "Explanation : There are 3,280 records in the dataset but no unique identifier for each assessment taken. The max number of attempts for all records is 1.We have to assume that there are duplicate records in the data. Assuming that each user_exam_id should be a unique record, there are 38 duplicate records in the data given that the count of unique exam ids is 3,242.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Insights : Conclusion and Recommendations¶\n",
    "\n",
    "1. The most popular courses are foundational courses like Introduction to Git, Introduction to Python, Intermediate Python, Introduction to Java 8 and Learning to Program with R. More resources can be devoted to marketing and developing these to improve the curriculum for users. Additionally, it looks like the primary user group of this portal are beginners or intermediate learners of data science tools and subjects.\n",
    "\n",
    "2. The least popular courses are Learning to Visualize Data with D3, Native Web Apps for Android, Nulls, Three-valued Logic and Missing Information, Operating Red Hat Enterprise Linux Servers and The Closed World Assumption. Consideration needs to be given to identify the reasons for low demand through customer surveys, for example customer type served, marketing, quality of exam and course and improvement made. Otherwise, the course should be shut down and have its resources shifted to new opportunities.\n",
    "\n",
    "3. There were 103 courses on the portal, this means on average each course generated 3 user assessments over the 2 months period. Courses which perform below this estimate can be considered below average and the opposite is true where the metric is above. A better metric can be devised if we had data on the length and complexity of the course which could also affect the number of user assessments taken over a given period of time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Issues\n",
    "\n",
    "A few data issues were identified during the exploration process. Namely:\n",
    "\n",
    "1. Lack of a unique record identifier or possible duplicate assessment records see explanation under Section 3: Data Exploration and assumption made.\n",
    "\n",
    "2. Errors in unnesting data in the sequences field of the data table due to multiple levels of nesting in arrays. \n",
    "\n",
    "I attempted to retrieve the data nested in arrays under the count field in order to find out how users are faring on these exams on average specifically by taking the correctly answered number of questions per user divided by the total number of questions per assessment.\n",
    "\n",
    "The structure of the sequence field is as follows:\n",
    "\n",
    "Map : [Questions] String, Array(Map(String,Boolean)) , [Count] String, Array(String)\n",
    "\n",
    "An attempt to unnest this using the explode function in PySpark SQL did not work due to the multiple nesting structure under more than one field components which have arrays. When the explode function is used, the exploded items which still have arrays under them lose the granular array data under them.\n",
    "\n",
    "`from pyspark.sql.functions import explode\n",
    "extracted_assess.select(extracted_assess.sequences,explode(extracted_assess.sequences)).show(truncate=False)`\n",
    "\n",
    "` a1=extracted_assess.select(explode(extracted_assess.sequences))\n",
    "a2=a1.select(a1.key,explode(a1.value))\n",
    "a3=a2.select(explode(a2.col))`\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
