raw_assess = spark.read.format("kafka").option("kafka.bootstrap.servers", "kafka:29092").option("subscribe","assessments").option("startingOffsets","earliest").option("endingOffsets","latest").load()
type(raw_assess)
raw_assess.cache()
raw_assess.printSchema()
assess_s= raw_assess.select(raw_assess.value.cast('string'))
assess_s.write.parquet("/tmp/assessments")
import sys
sys.stdout=open(sys.stdout.fileno(),mode='w',encoding='utf8',buffering=1)
import json
from pyspark.sql import Row
extracted_assess= assess_s.rdd.map(lambda x : json.loads(x.value)).toDF()
extracted_assess.show()
extracted_assess.registerTempTable('assess_temptab')
spark.sql("select base_exam_id, exam_name, user_exam_id, keen_id from assess_temptab limit 10").show()
spark.sql("select count(user_exam_id) from assess_temptab").show()
spark.sql("select count(distinct(exam_name)) from assess_temptab").show()
t1=spark.sql("select distinct(exam_name)), count(exam_name) from assess_temptab GROUP BY exam_name")
t1=spark.sql("select distinct(exam_name), count(exam_name) from assess_temptab GROUP BY exam_name")
t1.registerTempTable('t2')
t2.show()
t1=spark.sql("select distinct(exam_name), count(exam_name) as count from assess_temptab GROUP BY exam_name")
t1.registerTempTable('t2')
spark.sql("select exam_name,count from t2").show()
spark.sql("select exam_name,count from t2 ORDER BY count desc").show()
spark.sql("select exam_name,count from t2 ORDER BY count").show()
spark.sql("select exam_name, count from t2 CASE WHEN exam_name="Learning Git").show()
spark.sql("select exam_name, count from t2 WHERE exam_name="Learning Git").show()
spark.sql("select exam_name, count from t2 WHERE exam_name='Learning Git'").show()
extracted_assess.write.csv('mycsv.csv')
from pyspark.sql.functions import explode
extracted_assess.select(extracted_assess.sequences,explode(extracted_assess.sequences)).show(truncate=False)
a1=extracted_assess.select(extracted_assess.sequences,explode(extracted_assess.sequences))
a1.printSchema()
a2=a1.select(a1.key,explode(a1.value))
a2.show()
a2.printSchema()
a4=a1.select(
a3=a2.select(a2.col.key,a2.col.value)
a3.show()
a3.show(False)
a3.show(truncat=False)
a3.show(truncate=False)
spark.sql("select distinct(key) from a3").show()
a3.show()
a2.printSchema()
a2.show()
a1.printSchema()
a4=a1.select(a1.sequences.key,explode(a1.sequences.value),a1.key,explode(a1.value))
a4=a1.select(a1.sequences.key,explode(a1.sequences.value))
a4.show()
a4=a1.select(a1.key,explode(a1.value))
a4.show()
a4.col.show()
a4.col.value.show()
a4=a1.select(a1.sequences)
a4.show()
a4.sequences.value.show()
a4.sequences.counts.show()
extracted.assess.printSchema()
extracted_assess.printSchema()
c1=extracted_assess.select(extracted_assess.base_exam_id,extracted_assess.certification,extracted_assess.exam_name,extracted_assess.keen_created_at,extracted_assess.keen_id,extracted_assess.keen_timestamp,extracted_assess.max_attempts,extracted_assess.started_at,extract_assess.user_exam_id)
c1=extracted_assess.select(extracted_assess.base_exam_id,extracted_assess.certification,extracted_assess.exam_name,extracted_assess.keen_created_at,extracted_assess.keen_id,extracted_assess.keen_timestamp,extracted_assess.max_attempts,extracted_assess.started_at,extracted_assess.user_exam_id)
cq.write.csv('mycsv.csv')
c1.write.csv('mycsv.csv')
c1.write.csv('../w205/project-2-zilingh8/skh/mycsv.csv')
c1.show()
c1.write.csv('../tmp/mycsv.csv')
c1.write.csv('../skh/tmp/mycsv.csv')
c1.write.csv('../skh/mycsv.csv')
c1.printSchema()
c2=c1.select(to_date(c1.started_at,'yyyy-MM-dd HH:mm:ss')).alias('dt').collect()
from pyspark.sql.functions import to_timestamp
c2=c1.select(to_date(c1.started_at,'yyyy-MM-dd HH:mm:ss')).alias('dt').collect()
c2=c1.select(to_timestamp(c1.started_at,'yyyy-MM-dd HH:mm:ss')).alias('dt').collect()
c2.show()
cs
type(c2)
c2
c1.show()
c2= c1.select(c1.started_at.cast('timestamp'))
c2.show()
spark.sql("select min(started_at), max(started_at) from c2").show()
c2.show()
c2.printSchema()
spark.sql("select min(started_at) from c2").show()
c2.started_at.min().show()
type(c2)
spark.sql("select year(started_at) from c2").show()
c2.registerTempTable('c3')
spark.sql("select year(started_at) from c3").show()
spark.sql("select min(started_at),max(started_at) from c3").show()
raw_assess = spark.read.format("kafka").option("kafka.bootstrap.servers", "kafka:29092").option("subscribe","assessments").option("startingOffsets","earliest").option("endingOffsets","latest").load()
raw_assess.cache()
assess_s= raw_assess.select(raw_assess.value.cast('string'))
assess_s.write.parquet("/tmp/assessments")
assess_s.write.parquet("/tmp/assess")
import sys
sys.stdout=open(sys.stdout.fileno(),mode='w',encoding='utf8',buffering=1)
import json
from pyspark.sql import Row
extracted_assess= assess_s.rdd.map(lambda x : json.loads(x.value)).toDF()
extracted_assess.show()
extracted_assess.registerTempTable('assess_temptab')
spark.sql("select base_exam_id, exam_name, user_exam_id, keen_id from assess_temptab limit 10").show()
spark.sql("select count(user_exam_id) from assess_temptab").show()
spark.sql("select count(distinct(user_exam_id)) from assess_temptab").show()
spark.sql("select count(distinct(keen_id)) from assess_temptab").show()
spark.sql("select * from extracted_assess WHERE max_attempts>1").show()
spark.sql("select * from assess_temptab WHERE max_attempts>1").show()
d1=spark.sql("select exam_name, user_exam_id, count(user_exam_id) as count_uaid from assess_temptab GROUP BY exam_name, user_exam_id")
d1.show()
spark.sql("select * from assess_temptab where count_uaid>1").show()
spark.sql("select * from assess_temptab").show()
spark.sql("select * from d1 where count_uaid>1").show()
d1.show()
d1.registerTempTable('d2')
spark.sql("select * from d2 where count_uaid > 1 ").show()
 `spark.sql("select count(distinct(keen_id)) from assess_temptab").show()`
spark.sql("select count(distinct(keen_id)) from assess_temptab").show()
spark.sql("select base_exam_id, exam_name, user_exam_id, keen_id from assess_temptab limit 10").show()
spark.sql("select count(*) from assess_temptab").show()
spark.sql("select count(distinct(user_exam_id)) from assess_temptab").show()
d1=spark.sql("select exam_name, user_exam_id, count(user_exam_id) as count_uaid from assess_temptab GROUP BY exam_name, user_exam_id")
d1.registerTempTable('d2')
spark.sql("select * from d2 where count_uaid > 1 ").show()
spark.sql("select count(distinct(exam_name)) from assess_temptab").show()
t1=spark.sql("select distinct(exam_name), count(exam_name) as count from assess_temptab GROUP BY exam_name")
t1.registerTempTable('t2')
spark.sql("select exam_name,count from t2 ORDER BY count desc limit 5").show()
spark.sql("select exam_name from t2 ORDER BY count desc limit 5").show()
from pyspark.sql.functions import explode
extracted_assess.select(extracted_assess.sequences,explode(extracted_assess.sequences)).show(truncate=False)
extracted_assess.select(explode(extracted_assess.sequences)).show(truncate=False)
a1=extracted_assess.select(explode(extracted_assess.sequences))
a1.show()
a2=a1.select(a1.key,explode(a1.value))
a2.show()
a3=a2.select(a2.col.key,a2.col.value)
a3.show()
a2.show()
a3=a2.select(explode(a2.col))
a3.show()
a4=a1.select(a1.sequences.key,explode(a1.sequences.value),a1.key,explode(a1.value))
a4=a1.select(a1.sequences.key,explode(a1.sequences.value))
a1.show()
extracted_assess.sequences.show()
extracted_assess.sequences.key.show()
assess_temptab.sequences.key.show()
a5=assess_temptab.sequences.key
a5=extract_assess.sequences.key
extracted_assess.registerTempTable('assess_temptab')
a5=assess_temptab.select(assess_temptab.sequences.key)
a5=spark.sql("select sequences from assess_temptab")
a5.show()
a5.registerTempTable('a6')
a7=spark.sql("select explode(sequences) from a6")
a7.show()
a8=spark.sql("select sequences.key,sequences.value from a6")
a8.show()
